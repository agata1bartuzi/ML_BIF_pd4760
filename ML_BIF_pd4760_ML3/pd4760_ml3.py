# -*- coding: utf-8 -*-
"""pd4760_ML3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IRWjdNyZTsju_uBO552WLYbOCwZ2tNHB
"""

import pandas as pd
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix, classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns

# Dane wejściowe
data = {
    'TP53_expr': [2.1, 8.5, 1.8, 6.2, 7.9, 3.1, 9.2, 2.8, 6.8, 4.3, 7.5, 3.6, 5.2, 8.1, 1.9, 3.7, 5.9, 2.2, 6.5, 7.8],
    'BRCA1_expr': [3.4, 7.2, 2.5, 6.1, 6.8, 4.0, 7.9, 3.9, 6.6, 4.2, 7.0, 4.1, 5.8, 7.3, 2.2, 3.8, 5.5, 3.0, 6.0, 7.1],
    'TF_motifs': [2, 6, 1, 4, 5, 2, 6, 3, 5, 3, 5, 2, 3, 6, 1, 3, 4, 2, 5, 5],
    'MYC_expr': [1.5, 4.8, 1.2, 3.9, 5.1, 2.0, 4.9, 1.8, 3.8, 2.4, 4.5, 2.2, 3.1, 4.7, 1.3, 2.5, 3.7, 1.6, 4.1, 5.0],
    'CDKN2A_expr': [0.8, 2.3, 0.6, 1.7, 2.5, 1.0, 2.4, 0.9, 1.8, 1.2, 2.1, 1.1, 1.6, 2.2, 0.7, 1.3, 1.9, 0.8, 2.0, 2.6],
    'Promoter_methylation': [70, 20, 85, 30, 25, 65, 15, 75, 35, 60, 18, 55, 40, 22, 80, 58, 33, 68, 28, 19],
    'Chromatin_accessibility': [0.35, 0.92, 0.25, 0.78, 0.89, 0.42, 0.94, 0.31, 0.74, 0.45, 0.90, 0.50, 0.63, 0.91, 0.22, 0.47, 0.70, 0.38, 0.81, 0.93],
    'Cancer_status': [0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1]
}

df = pd.DataFrame(data)

# Przygotowanie danych
X = df.drop(columns='Cancer_status')
y = df['Cancer_status']

# Parametry do porównania
test_sizes = [0.25, 0.3, 0.5]
activations = ['relu', 'tanh']
hidden_layers = [(8, 4), (10,), (6, 6)]

# Lista wyników
results = []

# Pętla po wszystkich kombinacjach i podzial na zbior testowy i treningowy
for test_size in test_sizes:
    for activation in activations:
        for hidden_layer in hidden_layers:
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)

            model = MLPClassifier(hidden_layer_sizes=hidden_layer, activation=activation, max_iter=1000, random_state=42)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            results.append({
                'Test Size': test_size,
                'Activation': activation,
                'Hidden Layers': hidden_layer,
                'Accuracy': round(accuracy_score(y_test, y_pred), 2),
                'Precision': round(precision_score(y_test, y_pred, zero_division=0), 2),
                'Recall': round(recall_score(y_test, y_pred), 2),
                'F1 Score': round(f1_score(y_test, y_pred), 2)
            })

# Tworzenie tabeli wyników
results_df = pd.DataFrame(results)
print(results_df)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Tworzenie etykiety konfiguracji
results_df['Config'] = results_df['Test Size'].astype(str) + ' | ' + results_df['Activation'] + ' | ' + results_df['Hidden Layers'].astype(str)
results_df.set_index('Config', inplace=True)

# Tworzenie heatmapy
plt.figure(figsize=(12, 8))
sns.heatmap(results_df[['Accuracy', 'Precision', 'Recall', 'F1 Score']], annot=True, cmap='YlGnBu', fmt='.2f')
plt.title('Performance Metrics Heatmap for MLP Configurations')
plt.ylabel('Configuration')
plt.xlabel('Metric')
plt.tight_layout()
plt.show()

'''
Interpretacja wynikow:
Model, przy zastosowaniu funkcji aktywacyjnej ReLU, dziala doskonale juz na najmniejszym proponowanym zbiorze treningowym.
Natomiast funkcja Tanh daje satysfakcjonujace wyniki dopiero wtedy, gdy polowa probek znajduje sie w zbiorze treningowym,
a liczba ukrytych warstw sieci pierwszego rzedu wynosi wiecej niz 6.
'''